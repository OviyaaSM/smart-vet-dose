{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50abb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_dataset(n, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    RBC = np.random.normal(6.5, 1.0, n).clip(4.0, 9.0)\n",
    "    WBC = np.random.normal(10, 3.0, n).clip(4.0, 18.0)\n",
    "    HB  = np.random.normal(13, 2.5, n).clip(7.0, 20.0)\n",
    "    Platelets = np.random.normal(300, 80, n).clip(100, 700)\n",
    "    Creatinine = np.random.normal(1.0, 0.3, n).clip(0.3, 2.5)\n",
    "    Glucose    = np.random.normal(100, 25, n).clip(50, 200)\n",
    "    Dose = np.random.uniform(0.3, 1.3, n)\n",
    "\n",
    "    Outcome = (\n",
    "        (HB > 12) &\n",
    "        (RBC > 5.5) &\n",
    "        (Creatinine < 1.3) &\n",
    "        (Dose < 1.0)\n",
    "    ).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"RBC\": RBC,\n",
    "        \"WBC\": WBC,\n",
    "        \"HB\": HB,\n",
    "        \"Platelets\": Platelets,\n",
    "        \"Creatinine\": Creatinine,\n",
    "        \"Glucose\": Glucose,\n",
    "        \"Dose\": Dose,\n",
    "        \"Outcome\": Outcome\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85423ec9",
   "metadata": {},
   "source": [
    "**Notebook diagnostic:** Run the next code cell to show which Python executable this notebook kernel is using and whether numpy is importable. If numpy is missing, run the third cell to install it into this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d261333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "print('Notebook sys.executable:', sys.executable)\n",
    "try:\n",
    "    import numpy as np\n",
    "    print('Imported numpy, version:', np.__version__)\n",
    "except Exception as e:\n",
    "    print('Import numpy failed:', repr(e))\n",
    "print('\\npip show numpy:')\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'show', 'numpy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if numpy is missing in the kernel above.\n",
    "import sys, subprocess\n",
    "print('Installing numpy into:', sys.executable)\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'numpy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "# Number of samples\n",
    "n = 1000\n",
    "\n",
    "# Simulate hematology parameters within realistic biological ranges\n",
    "RBC = np.random.normal(loc=6.5, scale=1.0, size=n).clip(4.0, 9.0)          # 4–9 ×10^6/µL\n",
    "WBC = np.random.normal(loc=10, scale=3.0, size=n).clip(4.0, 18.0)          # 4–18 ×10^3/µL\n",
    "HB  = np.random.normal(loc=13, scale=2.5, size=n).clip(7.0, 20.0)          # 7–20 g/dL\n",
    "Platelets = np.random.normal(loc=300, scale=80, size=n).clip(100, 700)     # 100–700 ×10^3/µL\n",
    "Creatinine = np.random.normal(loc=1.0, scale=0.3, size=n).clip(0.3, 2.5)   # 0.3–2.5 mg/dL\n",
    "Glucose    = np.random.normal(loc=100, scale=25, size=n).clip(50, 200)     # 50–200 mg/dL\n",
    "\n",
    "# Dose varies between low and high doses\n",
    "Dose = np.random.uniform(0.3, 1.3, n)\n",
    "\n",
    "# More realistic outcome rule (multifactor)\n",
    "Outcome = (\n",
    "    (HB > 12) & \n",
    "    (RBC > 5.5) &\n",
    "    (Creatinine < 1.3) &\n",
    "    (Dose < 1.0)\n",
    ").astype(int)\n",
    "\n",
    "# Construct dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"RBC\": RBC,\n",
    "    \"WBC\": WBC,\n",
    "    \"HB\": HB,\n",
    "    \"Platelets\": Platelets,\n",
    "    \"Creatinine\": Creatinine,\n",
    "    \"Glucose\": Glucose,\n",
    "    \"Dose\": Dose,\n",
    "    \"Outcome\": Outcome\n",
    "})\n",
    "\n",
    "df.head(), df.shape\n",
    "df.to_csv(\"smart_vet_dose_1000.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ade95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "accuracy_log = accuracy_score(y_test, y_pred)\n",
    "accuracy_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3518bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, rf_pred)\n",
    "accuracy_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=4)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, xgb_pred)\n",
    "accuracy_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed45ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate_models(df):\n",
    "    X = df.drop(\"Outcome\", axis=1)\n",
    "    y = df[\"Outcome\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    lr_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "    results[\"Logistic Regression\"] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, lr_pred),\n",
    "        \"Precision\": precision_score(y_test, lr_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, lr_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, lr_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "\n",
    "    results[\"Random Forest\"] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, rf_pred),\n",
    "        \"Precision\": precision_score(y_test, rf_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, rf_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, rf_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=4, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "    results[\"XGBoost\"] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, xgb_pred),\n",
    "        \"Precision\": precision_score(y_test, xgb_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, xgb_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_test, xgb_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for different dataset sizes\n",
    "import pandas as pd\n",
    "\n",
    "dataset_sizes = [400, 700, 1000, 1250]\n",
    "final_results = []\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    df = generate_dataset(size)\n",
    "    model_results = evaluate_models(df)\n",
    "\n",
    "    for model, metrics in model_results.items():\n",
    "        final_results.append({\n",
    "            \"Dataset Size\": size,\n",
    "            \"Model\": model,\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(final_results)\n",
    "# Pivot for nicer display\n",
    "results_df_pivot = results_df.pivot_table(index=[\"Dataset Size\", \"Model\"], values=[\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]).reset_index()\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"model_results.csv\", index=False)\n",
    "\n",
    "results_df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results summary\n",
    "import pandas as pd\n",
    "results_df = pd.read_csv(\"model_results.csv\")\n",
    "results_df_pivot = results_df.pivot_table(index=[\"Dataset Size\", \"Model\"], values=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"]).reset_index()\n",
    "results_df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35024b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick numeric summary of results\n",
    "import pandas as pd\n",
    "results_df = pd.read_csv('model_results.csv')\n",
    "print('Rows:', len(results_df))\n",
    "print(results_df.groupby('Model')['Accuracy'].agg(['mean','min','max']).round(3))\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations: ensure plotting libs installed, then bar plots of metrics and confusion matrix for Random Forest on 1000-sample dataset\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    print('Installing matplotlib and seaborn...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'matplotlib', 'seaborn'])\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# read results\n",
    "results_df = pd.read_csv('model_results.csv')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=results_df, x='Model', y='Accuracy')\n",
    "plt.title('Accuracy by Model (all sizes)')\n",
    "plt.ylim(0, 1.02)\n",
    "plt.show()\n",
    "\n",
    "# Show metrics per dataset size\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "fig, axes = plt.subplots(1, len(metrics), figsize=(18, 4))\n",
    "for i, metric in enumerate(metrics):\n",
    "    sns.barplot(ax=axes[i], data=results_df, x='Dataset Size', y=metric, hue='Model')\n",
    "    axes[i].set_title(metric)\n",
    "    axes[i].set_ylim(0, 1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for Random Forest on 1000-sample dataset\n",
    "df_1000 = generate_dataset(1000)\n",
    "X = df_1000.drop('Outcome', axis=1)\n",
    "y = df_1000['Outcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix — Random Forest (n=1000)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
